{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/azizbarank/distilroberta-base-sst-2-distilled/blob/main/knowledge_distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xknRaAfngCT1"
   },
   "source": [
    "## Installing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "285QJOlK9Z2W",
    "outputId": "c4169aea-40e4-4e75-8c29-d3faa58ff92b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.2.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.49.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (2.3.4-1).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-460\n",
      "Use 'sudo apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets tensorboard\n",
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFefM0LdgJkH"
   },
   "source": [
    "## Chhosing our \"teacher\" and \"student\" models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wrQyPjcl9-Ol"
   },
   "outputs": [],
   "source": [
    "student = \"distilroberta-base\" # this is just placeholder ignore\n",
    "teacher = \"klue/roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxo4XGgRgQhO"
   },
   "source": [
    "## Loading our SST-2 part of the GLUE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "e9acdb1a1d2643819339b8f0cf3cecd1",
      "135dbaaa41a946ca899dc8164bca3e44",
      "09db8fe0b55443fdab2ac54e84bb7ff5",
      "b591e297adff46fa8df014fe9192caa3",
      "8c2682025ce14a88b39dbfc826e33954",
      "5ad3705618644447bfcfdb9b65271883",
      "b71e32972c104b53a669ec7591e557ff",
      "861ab3f25cb4479f9ace3978dc315ecf",
      "52eabebd19834610890b10347acaf05a",
      "e39bf65dc61a4836805a0a8bd23503a3",
      "a864568c3b89405cb3ca8e36180f9e61"
     ]
    },
    "id": "ZhFP95R3-xs-",
    "outputId": "f5248a08-07c1-4640-f397-a370c2a60e53",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset klue (/home/hanjuncho/.cache/huggingface/datasets/klue/re/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a8d71a2437452cb97892f885754caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"re\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = dataset[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no_relation': 0,\n",
       " 'org:dissolved': 0,\n",
       " 'org:founded': 0,\n",
       " 'org:place_of_headquarters': 0,\n",
       " 'org:alternate_names': 0,\n",
       " 'org:member_of': 0,\n",
       " 'org:members': 0,\n",
       " 'org:political/religious_affiliation': 0,\n",
       " 'org:product': 0,\n",
       " 'org:founded_by': 0,\n",
       " 'org:top_members/employees': 0,\n",
       " 'org:number_of_employees/members': 0,\n",
       " 'per:date_of_birth': 0,\n",
       " 'per:date_of_death': 0,\n",
       " 'per:place_of_birth': 0,\n",
       " 'per:place_of_death': 0,\n",
       " 'per:place_of_residence': 0,\n",
       " 'per:origin': 0,\n",
       " 'per:employee_of': 0,\n",
       " 'per:schools_attended': 0,\n",
       " 'per:alternate_names': 0,\n",
       " 'per:parents': 0,\n",
       " 'per:children': 0,\n",
       " 'per:siblings': 0,\n",
       " 'per:spouse': 0,\n",
       " 'per:other_family': 0,\n",
       " 'per:colleagues': 0,\n",
       " 'per:product': 0,\n",
       " 'per:religion': 0,\n",
       " 'per:title': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(name)):\n",
    "#     dic[name[i]] = 0\n",
    "# dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no_relation': 9534,\n",
       " 'org:dissolved': 66,\n",
       " 'org:founded': 450,\n",
       " 'org:place_of_headquarters': 1195,\n",
       " 'org:alternate_names': 1320,\n",
       " 'org:member_of': 1866,\n",
       " 'org:members': 420,\n",
       " 'org:political/religious_affiliation': 98,\n",
       " 'org:product': 380,\n",
       " 'org:founded_by': 155,\n",
       " 'org:top_members/employees': 4284,\n",
       " 'org:number_of_employees/members': 48,\n",
       " 'per:date_of_birth': 1130,\n",
       " 'per:date_of_death': 418,\n",
       " 'per:place_of_birth': 166,\n",
       " 'per:place_of_death': 40,\n",
       " 'per:place_of_residence': 193,\n",
       " 'per:origin': 1234,\n",
       " 'per:employee_of': 3573,\n",
       " 'per:schools_attended': 82,\n",
       " 'per:alternate_names': 1001,\n",
       " 'per:parents': 520,\n",
       " 'per:children': 304,\n",
       " 'per:siblings': 136,\n",
       " 'per:spouse': 795,\n",
       " 'per:other_family': 190,\n",
       " 'per:colleagues': 534,\n",
       " 'per:product': 139,\n",
       " 'per:religion': 96,\n",
       " 'per:title': 2103}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(dataset[\"train\"])):\n",
    "#     dic[name[dataset[\"train\"][\"label\"][i]]] += 1\n",
    "    \n",
    "# dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM5nz3GOgXoG"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlAARAR8ge9r"
   },
   "source": [
    "### Initiating the tokenizer of our student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "9e2728489569432394255a3bb3a80247",
      "af6bdd67d22a42f0be80845511720ae0",
      "3b32dbed334941b881ae0e563d7c40ce",
      "6abc6629a8b0412c9b09ad02c3376127",
      "8722d84268664df5a8012a83afd575f3",
      "3ddd98a8524042adba2bbaab1dbf60af",
      "8a1e9c4517dc463ba78b91b16431008a",
      "0a8902cb735d4f658fd387204149ace6",
      "e25584843ddd4a4484ec94e7e6c0f6a0",
      "155517c19bc24d599b068af7e28ad8cf",
      "3838ac5a07244d7da893c8ba71b21fd9"
     ]
    },
    "id": "-pTJHVMv-7fz",
    "outputId": "d5047064-b098-435b-938c-ffd0c4a3ed9b"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191,
     "referenced_widgets": [
      "ea941579121247548fdf7774079f2f5b",
      "0faf64a0c2af4340ac3b0eed24732d5a",
      "cde2d6e48a7748ca840c745e52fa06c2",
      "7874b0b368c24ef7b15fac3a4ed83582",
      "a255def76d9b4c9db5d9cc7385fc9e6d",
      "b0614b4ade314c5c8073193bb1d3408f",
      "38b21b8459234a42b8fb69752644d48a",
      "5ffb8a1db11a43cb8602ec7af8c55f77",
      "5293a6fc609849eda02622c6fa5b2167",
      "c9403580e4524754980170c5f1130bd9",
      "1e5da5a29aaa4700bcdbdfc768b33aaf"
     ]
    },
    "id": "4W4oB3mzHJlb",
    "outputId": "60e0f718-85e2-4b4b-ae18-48e52f6453cb"
   },
   "outputs": [],
   "source": [
    "def add_entity_tokens(sentence, object_entity, subject_entity):\n",
    "    obj_start_idx, obj_end_idx = object_entity['start_idx'], object_entity['end_idx']\n",
    "    subj_start_idx, subj_end_idx = subject_entity['start_idx'], subject_entity['end_idx']\n",
    "    \n",
    "    if obj_start_idx < subj_start_idx:\n",
    "        new_sentence = sentence[:obj_start_idx] + '<obj>' + sentence[obj_start_idx:obj_end_idx+1] + '</obj>' + \\\n",
    "                       sentence[obj_end_idx+1:subj_start_idx] + '<subj>' + sentence[subj_start_idx:subj_end_idx+1] + \\\n",
    "                       '</subj>' + sentence[subj_end_idx+1:]\n",
    "    else:\n",
    "        new_sentence = sentence[:subj_start_idx] + '<subj>' + sentence[subj_start_idx:subj_end_idx+1] + '</subj>' + \\\n",
    "                       sentence[subj_end_idx+1:obj_start_idx] + '<obj>' + sentence[obj_start_idx:obj_end_idx+1] + \\\n",
    "                       '</obj>' + sentence[obj_end_idx+1:]\n",
    "    \n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "def read_klue_re(dataset):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    if isinstance(dataset, Dataset):\n",
    "        for data in dataset:\n",
    "            sentence = add_entity_tokens(data['sentence'], data['object_entity'], data['subject_entity'])\n",
    "            sentences.append(sentence)\n",
    "            labels.append(data['label'])\n",
    "    \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validationÎç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑú sentenceÏôÄ labelÎßå Ï†ÄÏû•.\n",
    "train_sentences, train_labels = read_klue_re(dataset['train'])\n",
    "val_sentences, val_labels = read_klue_re(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_encoding = tokenizer(dataset['train'][0]['sentence'],\n",
    "                        max_length=128,\n",
    "                        padding='max_length',\n",
    "                        truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Token Ï∂îÍ∞Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_special_tokens = {'additional_special_tokens': ['<obj>', '</obj>', '<subj>', '</subj>']}\n",
    "num_additional_special_tokens = tokenizer.add_special_tokens(entity_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KlueReDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels, max_length=128):\n",
    "        self.encodings = tokenizer(sentences,\n",
    "                                   max_length=max_length,\n",
    "                                   padding='max_length',\n",
    "                                   truncation=True)\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGE72or9gsGn"
   },
   "source": [
    "## Creating our Knowledge Distillation Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JayX6RkQ_GZf"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GdeAjnlUERab"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self._move_model_to_device(self.teacher,self.model.device)\n",
    "        self.teacher.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        # compute student output\n",
    "        outputs_student = model(**inputs)\n",
    "        student_loss=outputs_student.loss\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher(**inputs)\n",
    "\n",
    "        # assert size\n",
    "        assert outputs_student.logits.size() == outputs_teacher.logits.size()\n",
    "\n",
    "        # compute distillation loss and soften probabilities\n",
    "        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "#         cos_loss_function = nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
    "        \n",
    "        loss_logits = (loss_function(\n",
    "            F.log_softmax(outputs_student.logits / self.args.temperature, dim=-1),\n",
    "            F.softmax(outputs_teacher.logits / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n",
    "        # return weighted student loss\n",
    "        loss = self.args.alpha * student_loss + (1. - self.args.alpha) * loss_logits\n",
    "        return (loss, outputs_student) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of9nToqqhF9Y"
   },
   "source": [
    "## Defining the Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZl-LLhKJLIZ",
    "outputId": "e6d2270a-02d1-4074-f546-b536697d30fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_518268/923010648.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"micro\")\n",
    "    return {\n",
    "        \"accuracy\": acc[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig\n",
    "from transformers import AutoConfig, AutoModel\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model\n",
    "num_labels = 30\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = KlueReDataset(tokenizer, train_sentences, train_labels)\n",
    "val_dataset = KlueReDataset(tokenizer, val_sentences, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/hanjuncho/.cache/huggingface/hub/models--klue--roberta-base/snapshots/67dd433d36ebc66a42c9aaa85abcf8d2620e41d9/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/hanjuncho/.cache/huggingface/hub/models--klue--roberta-base/snapshots/67dd433d36ebc66a42c9aaa85abcf8d2620e41d9/pytorch_model.bin\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32004, 768)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher,\n",
    "    num_labels=num_labels, #30\n",
    ")\n",
    "teacher_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32004, 768)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, DataCollatorWithPadding\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "teacher_model = AutoModel.from_pretrained(\n",
    "    teacher,\n",
    "    num_labels=num_labels, #30\n",
    ")\n",
    "teacher_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # checkpoint\n",
    "    output_dir='./models/',\n",
    "    # overwrite_output_dir=True,\n",
    "\n",
    "    # Model Save & Load\n",
    "    save_strategy = \"epoch\", # 'steps'\n",
    "    load_best_model_at_end=True,\n",
    "    # save_steps = 500,\n",
    "\n",
    "\n",
    "    # Dataset\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate=2e-5, # 5e-5\n",
    "    weight_decay=0.01,  # 0\n",
    "    # warmup_steps=200,b\n",
    "\n",
    "    # Resularization\n",
    "    # max_grad_norm = 1.0,\n",
    "    # label_smoothing_factor=0.1,\n",
    "\n",
    "\n",
    "    # Evaluation \n",
    "    metric_for_best_model='eval_f1',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "\n",
    "    # Randomness\n",
    "    seed=33,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./klue-roberta-base-re/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32004\n",
      "}\n",
      "\n",
      "loading weights file ./klue-roberta-base-re/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./klue-roberta-base-re.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "teacher_model = teacher_model.from_pretrained('./klue-roberta-base-re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_model = teacher_model\n",
    "\n",
    "parameters_to_prune = ()\n",
    "for i in range(12):\n",
    "    parameters_to_prune += (\n",
    "        (prune_model.roberta.encoder.layer[i].attention.self.key, 'weight'),\n",
    "        (prune_model.roberta.encoder.layer[i].attention.self.query, 'weight'),\n",
    "        (prune_model.roberta.encoder.layer[i].attention.self.value, 'weight'),\n",
    "    )\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32004, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=30, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "477912.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(torch.sum(prune_model.roberta.encoder.layer[0].attention.self.key.weight==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196133.0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(torch.sum(teacher_model.roberta.encoder.layer[0].attention.self.key.weight==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in Layer 1-th key weight: 33.25%\n",
      "Sparsity in Layer 1-th query weightt: 34.30%\n",
      "Sparsity in Layer 1-th value weight: 52.34%\n",
      "\n",
      "Sparsity in Layer 2-th key weight: 37.52%\n",
      "Sparsity in Layer 2-th query weightt: 37.97%\n",
      "Sparsity in Layer 2-th value weight: 51.71%\n",
      "\n",
      "Sparsity in Layer 3-th key weight: 38.17%\n",
      "Sparsity in Layer 3-th query weightt: 38.28%\n",
      "Sparsity in Layer 3-th value weight: 50.36%\n",
      "\n",
      "Sparsity in Layer 4-th key weight: 36.78%\n",
      "Sparsity in Layer 4-th query weightt: 36.83%\n",
      "Sparsity in Layer 4-th value weight: 49.73%\n",
      "\n",
      "Sparsity in Layer 5-th key weight: 37.39%\n",
      "Sparsity in Layer 5-th query weightt: 36.89%\n",
      "Sparsity in Layer 5-th value weight: 47.56%\n",
      "\n",
      "Sparsity in Layer 6-th key weight: 36.52%\n",
      "Sparsity in Layer 6-th query weightt: 35.75%\n",
      "Sparsity in Layer 6-th value weight: 50.10%\n",
      "\n",
      "Sparsity in Layer 7-th key weight: 36.01%\n",
      "Sparsity in Layer 7-th query weightt: 35.88%\n",
      "Sparsity in Layer 7-th value weight: 47.20%\n",
      "\n",
      "Sparsity in Layer 8-th key weight: 36.28%\n",
      "Sparsity in Layer 8-th query weightt: 35.60%\n",
      "Sparsity in Layer 8-th value weight: 45.32%\n",
      "\n",
      "Sparsity in Layer 9-th key weight: 36.21%\n",
      "Sparsity in Layer 9-th query weightt: 35.16%\n",
      "Sparsity in Layer 9-th value weight: 45.82%\n",
      "\n",
      "Sparsity in Layer 10-th key weight: 35.04%\n",
      "Sparsity in Layer 10-th query weightt: 33.77%\n",
      "Sparsity in Layer 10-th value weight: 48.51%\n",
      "\n",
      "Sparsity in Layer 11-th key weight: 35.13%\n",
      "Sparsity in Layer 11-th query weightt: 34.14%\n",
      "Sparsity in Layer 11-th value weight: 45.87%\n",
      "\n",
      "Sparsity in Layer 12-th key weight: 35.10%\n",
      "Sparsity in Layer 12-th query weightt: 34.30%\n",
      "Sparsity in Layer 12-th value weight: 43.19%\n",
      "\n",
      "Global sparsity: 40.00%\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    print(\n",
    "        \"Sparsity in Layer {}-th key weight: {:.2f}%\".format(\n",
    "            i+1,\n",
    "            100. * float(torch.sum(prune_model.roberta.encoder.layer[i].attention.self.key.weight == 0))\n",
    "            / float(prune_model.roberta.encoder.layer[i].attention.self.key.weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in Layer {}-th query weightt: {:.2f}%\".format(\n",
    "            i+1,\n",
    "            100. * float(torch.sum(prune_model.roberta.encoder.layer[i].attention.self.query.weight == 0))\n",
    "            / float(prune_model.roberta.encoder.layer[i].attention.self.query.weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Sparsity in Layer {}-th value weight: {:.2f}%\".format(\n",
    "            i+1,\n",
    "            100. * float(torch.sum(prune_model.roberta.encoder.layer[i].attention.self.value.weight == 0))\n",
    "            / float(prune_model.roberta.encoder.layer[i].attention.self.value.weight.nelement())\n",
    "        )\n",
    "    )\n",
    "    print()\n",
    "\n",
    "numerator, denominator = 0, 0\n",
    "for i in range(12):\n",
    "    numerator += torch.sum(prune_model.roberta.encoder.layer[i].attention.self.key.weight == 0)\n",
    "    numerator += torch.sum(prune_model.roberta.encoder.layer[i].attention.self.query.weight == 0)\n",
    "    numerator += torch.sum(prune_model.roberta.encoder.layer[i].attention.self.value.weight == 0)\n",
    "\n",
    "    denominator += prune_model.roberta.encoder.layer[i].attention.self.key.weight.nelement()\n",
    "    denominator += prune_model.roberta.encoder.layer[i].attention.self.query.weight.nelement()\n",
    "    denominator += prune_model.roberta.encoder.layer[i].attention.self.value.weight.nelement()\n",
    "    \n",
    "print(\"Global sparsity: {:.2f}%\".format(100. * float(numerator) / float(denominator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    teacher_model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = Trainer(\n",
    "    prune_model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/61 06:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8514652848243713,\n",
       " 'eval_accuracy': 0.7384417256922087,\n",
       " 'eval_f1': 0.7384417256922087,\n",
       " 'eval_runtime': 419.1337,\n",
       " 'eval_samples_per_second': 18.526,\n",
       " 'eval_steps_per_second': 0.146}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/61 06:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.915097177028656,\n",
       " 'eval_accuracy': 0.719768190598841,\n",
       " 'eval_f1': 0.7197681905988411,\n",
       " 'eval_runtime': 410.6185,\n",
       " 'eval_samples_per_second': 18.91,\n",
       " 'eval_steps_per_second': 0.149}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 32470\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1270\n",
      "  Number of trainable parameters = 110644254\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='1270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5/1270 01:26 < 10:08:39, 0.03 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/trainer.py:2526\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2526\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/wandb_torch.py:282\u001b[0m, in \u001b[0;36mTorchHistory._hook_variable_gradient_stats.<locals>.<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_tensor_stats(grad\u001b[38;5;241m.\u001b[39mdata, name)\n\u001b[0;32m--> 282\u001b[0m handle \u001b[38;5;241m=\u001b[39m var\u001b[38;5;241m.\u001b[39mregister_hook(\u001b[38;5;28;01mlambda\u001b[39;00m grad: _callback(grad, log_track))\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_handles[name] \u001b[38;5;241m=\u001b[39m handle\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/61 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9150362610816956,\n",
       " 'eval_accuracy': 0.719768190598841,\n",
       " 'eval_f1': 0.7197681905988411,\n",
       " 'eval_runtime': 9.534,\n",
       " 'eval_samples_per_second': 814.456,\n",
       " 'eval_steps_per_second': 6.398,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./klue-roberta-base-re/tokenizer_config.json\n",
      "Special tokens file saved in ./klue-roberta-base-re/special_tokens_map.json\n",
      "Configuration saved in ./klue-roberta-base-re/config.json\n",
      "Model weights saved in ./klue-roberta-base-re/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./klue-roberta-base-re')\n",
    "teacher_model.save_pretrained('./klue-roberta-base-re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./klue-roberta-base-re/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32004\n",
      "}\n",
      "\n",
      "loading weights file ./klue-roberta-base-re/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./klue-roberta-base-re.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "teacher_model = teacher_model.from_pretrained('./klue-roberta-base-re')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zouXYIx0hnd0"
   },
   "source": [
    "## Defining the Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig\n",
    "from transformers import AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hv-Et_22JOgk",
    "outputId": "c9d89407-8216-4807-dcc0-5afa607f6a04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file config.json from cache at /home/hanjuncho/.cache/huggingface/hub/models--klue--roberta-base/snapshots/67dd433d36ebc66a42c9aaa85abcf8d2620e41d9/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/hanjuncho/.cache/huggingface/hub/models--klue--roberta-base/snapshots/67dd433d36ebc66a42c9aaa85abcf8d2620e41d9/pytorch_model.bin\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /home/hanjuncho/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/hanjuncho/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32004, 768)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# training arguments\n",
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=\"distilroberta-base-sst2-distilled\",\n",
    "    num_train_epochs=7, per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128, fp16=True, \n",
    "    learning_rate=6e-5, seed=33, \n",
    "    logging_dir=f\"distilroberta-base-sst2-distilled/logs\",\n",
    "    logging_strategy=\"epoch\", evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", save_total_limit=2, \n",
    "    load_best_model_at_end=True, metric_for_best_model=\"eval_f1\", \n",
    "    report_to=\"tensorboard\", push_to_hub=False,\n",
    "    alpha=0.5, temperature=4.0\n",
    "    )\n",
    "\n",
    "# data_collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# # teacher model\n",
    "teacher_model = teacher_model.from_pretrained(\n",
    "    teacher,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "# student model\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    student,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "student_model.resize_token_embeddings(len(tokenizer))\n",
    "#teacher_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_weights = []\n",
    "for i, p in enumerate(student_model.parameters()):\n",
    "    student_weights.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized one layer out of two\n",
    "teacher_weights = []\n",
    "for i, p in enumerate(teacher_model.parameters()):\n",
    "    teacher_weights.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0032,  0.0120,  0.0437,  ..., -0.0044,  0.0152,  0.0163],\n",
       "        [ 0.0298,  0.0116,  0.0083,  ...,  0.0177, -0.0170,  0.0026],\n",
       "        [-0.0386,  0.0028,  0.0212,  ..., -0.0239, -0.0213,  0.0151],\n",
       "        ...,\n",
       "        [ 0.0191, -0.0081, -0.0129,  ...,  0.0195, -0.0090, -0.0066],\n",
       "        [-0.0281,  0.0092, -0.0060,  ..., -0.0333,  0.0045,  0.0329],\n",
       "        [-0.0148, -0.0076, -0.0034,  ..., -0.0141, -0.0082,  0.0278]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First and last layers\n",
    "student_weights[0].data.copy_(teacher_weights[0].data)\n",
    "student_weights[1].data.copy_(teacher_weights[1].data)\n",
    "student_weights[2].data.copy_(teacher_weights[2].data)\n",
    "student_weights[-1].data.copy_(teacher_weights[-1].data)\n",
    "student_weights[-2].data.copy_(teacher_weights[-2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110621184"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_weights = []\n",
    "for i, p in enumerate(student_model.parameters()):\n",
    "    student_weights.append(p)\n",
    "\n",
    "# initialized one layer out of two\n",
    "teacher_weights = []\n",
    "for i, p in enumerate(teacher_model.parameters()):\n",
    "    teacher_weights.append(p)\n",
    "\n",
    "# First and last layers\n",
    "student_weights[0].data.copy_(teacher_weights[0].data)\n",
    "student_weights[1].data.copy_(teacher_weights[1].data)\n",
    "student_weights[2].data.copy_(teacher_weights[2].data)\n",
    "student_weights[-1].data.copy_(teacher_weights[-1].data)\n",
    "student_weights[-2].data.copy_(teacher_weights[-2].data)\n",
    "\n",
    "base = 3\n",
    "for i in range(12):\n",
    "    if i % 2 == 1:\n",
    "        std_idx = i // 2\n",
    "        for j in range(16):\n",
    "            student_weights[base+std_idx*16+j].data.copy_(teacher_weights[base+i*16+j].data)\n",
    "            \n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters())[:-2]:\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(student_model)\n",
    "get_n_params(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 3\n",
    "for i in range(12):\n",
    "    if i % 2 == 1:\n",
    "        std_idx = i // 2\n",
    "        for j in range(16):\n",
    "            student_weights[base+std_idx*16+j].data.copy_(teacher_weights[base+i*16+j].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters())[:-2]:\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68093952"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_params(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110621184"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_params(teacher_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWpFJ4gKiEri"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "### to continue learning\n",
    "\n",
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=\"distilroberta-base-sst2-distilled\",\n",
    "    num_train_epochs=7, per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128, fp16=True, \n",
    "    learning_rate=6e-5, seed=33, \n",
    "    logging_dir=f\"distilroberta-base-sst2-distilled/logs\",\n",
    "    logging_strategy=\"epoch\", evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", save_total_limit=2, \n",
    "    load_best_model_at_end=True, metric_for_best_model=\"eval_f1\", \n",
    "    report_to=\"tensorboard\", push_to_hub=False,\n",
    "    alpha=0.5, temperature=4.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C94kC2PqLaP_",
    "outputId": "521b7405-56b7-4d3d-eac5-3e7d74cc0242"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = DistillationTrainer(\n",
    "    student_model,\n",
    "    training_args,\n",
    "    teacher_model=teacher_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GLvdvmOFLeuG",
    "outputId": "a7894862-1460-4d79-e24b-a635e2f1ee8d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanjuncho/anaconda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 32470\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1778\n",
      "  Number of trainable parameters = 68117022\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1778' max='1778' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1778/1778 09:54, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.122600</td>\n",
       "      <td>1.125819</td>\n",
       "      <td>0.505473</td>\n",
       "      <td>0.505473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.785100</td>\n",
       "      <td>0.994270</td>\n",
       "      <td>0.578364</td>\n",
       "      <td>0.578364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.677700</td>\n",
       "      <td>1.033804</td>\n",
       "      <td>0.572054</td>\n",
       "      <td>0.572054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>1.082392</td>\n",
       "      <td>0.541017</td>\n",
       "      <td>0.541017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.553000</td>\n",
       "      <td>1.089619</td>\n",
       "      <td>0.560335</td>\n",
       "      <td>0.560335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.513500</td>\n",
       "      <td>1.062945</td>\n",
       "      <td>0.584932</td>\n",
       "      <td>0.584932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.488500</td>\n",
       "      <td>1.115968</td>\n",
       "      <td>0.565357</td>\n",
       "      <td>0.565357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-254\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-254/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-254/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-254/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-254/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-508\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-508/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-508/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-508/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-508/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-254] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-762\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-762/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-762/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-762/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-762/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-1778] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-1016\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-1016/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-1016/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-1016/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-1016/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-762] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-1270\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-1270/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-1270/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-1270/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-1270/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-1016] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-1524\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-1524/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-1524/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-1524/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-1524/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-508] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-1778\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-1778/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-1778/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-1778/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-1778/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-1270] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distilroberta-base-sst2-distilled/checkpoint-1524 (score: 0.5849323889246619).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1778, training_loss=0.6777900635726809, metrics={'train_runtime': 594.9921, 'train_samples_per_second': 382.005, 'train_steps_per_second': 2.988, 'total_flos': 7530887358489600.0, 'train_loss': 0.6777900635726809, 'epoch': 7.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/o init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/61 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.54940664768219,\n",
       " 'eval_accuracy': 0.5344494526722473,\n",
       " 'eval_f1': 0.5344494526722473,\n",
       " 'eval_runtime': 10.2213,\n",
       " 'eval_samples_per_second': 759.69,\n",
       " 'eval_steps_per_second': 5.968,\n",
       " 'epoch': 7.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearly Decaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self._move_model_to_device(self.teacher,self.model.device)\n",
    "        self.teacher.eval()\n",
    "        self.step = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        self.step += 1\n",
    "        # compute student output\n",
    "        outputs_student = model(**inputs)\n",
    "        student_loss=outputs_student.loss\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher(**inputs)\n",
    "\n",
    "        # assert size\n",
    "        assert outputs_student.logits.size() == outputs_teacher.logits.size()\n",
    "\n",
    "        # compute distillation loss and soften probabilities\n",
    "        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "#         cos_loss_function = nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
    "        \n",
    "        loss_logits = (loss_function(\n",
    "            F.log_softmax(outputs_student.logits / self.args.temperature, dim=-1),\n",
    "            F.softmax(outputs_teacher.logits / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n",
    "        # return weighted student loss\n",
    "        loss = 1.*(2499-self.step)/2499 * student_loss + (1. - 1.*(2499-self.step)/2499) * loss_logits\n",
    "        return (loss, outputs_student) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./klue-roberta-base-re/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32004\n",
      "}\n",
      "\n",
      "loading weights file ./klue-roberta-base-re/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./klue-roberta-base-re.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "teacher_model = teacher_model.from_pretrained('./klue-roberta-base-re')\n",
    "#teacher_model = teacher_model.from_pretrained(\"klue/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file config.json from cache at /home/hanjuncho/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/hanjuncho/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32004, 768)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# training arguments\n",
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=\"distilroberta-base-sst2-distilled\",\n",
    "    num_train_epochs=7, per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128, fp16=True, \n",
    "    learning_rate=6e-5, seed=33, \n",
    "    logging_dir=f\"distilroberta-base-sst2-distilled/logs\",\n",
    "    logging_strategy=\"epoch\", evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", save_total_limit=2, \n",
    "    load_best_model_at_end=True, metric_for_best_model=\"eval_f1\", \n",
    "    report_to=\"tensorboard\", push_to_hub=False,\n",
    "    alpha=0.5, temperature=4.0\n",
    "    )\n",
    "\n",
    "# data_collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# student model\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    student,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "student_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do weight init !!! ÏúÑÏóê ÏûàÎäî ÏΩîÎìú Ïã§ÌñâÌï†Í≤É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110621184"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_weights = []\n",
    "for i, p in enumerate(student_model.parameters()):\n",
    "    student_weights.append(p)\n",
    "\n",
    "# initialized one layer out of two\n",
    "teacher_weights = []\n",
    "for i, p in enumerate(teacher_model.parameters()):\n",
    "    teacher_weights.append(p)\n",
    "\n",
    "# First and last layers\n",
    "student_weights[0].data.copy_(teacher_weights[0].data)\n",
    "student_weights[1].data.copy_(teacher_weights[1].data)\n",
    "student_weights[2].data.copy_(teacher_weights[2].data)\n",
    "student_weights[-1].data.copy_(teacher_weights[-1].data)\n",
    "student_weights[-2].data.copy_(teacher_weights[-2].data)\n",
    "\n",
    "base = 3\n",
    "for i in range(12):\n",
    "    if i % 2 == 1:\n",
    "        std_idx = i // 2\n",
    "        for j in range(16):\n",
    "            student_weights[base+std_idx*16+j].data.copy_(teacher_weights[base+i*16+j].data)\n",
    "            \n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters())[:-2]:\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(student_model)\n",
    "get_n_params(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "### to continue learning\n",
    "\n",
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=\"distilroberta-base-sst2-distilled\",\n",
    "    num_train_epochs=7, per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128, fp16=True, \n",
    "    learning_rate=6e-5, seed=33, \n",
    "    logging_dir=f\"distilroberta-base-sst2-distilled/logs\",\n",
    "    logging_strategy=\"epoch\", evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", save_total_limit=2, \n",
    "    load_best_model_at_end=True, metric_for_best_model=\"eval_f1\", \n",
    "    report_to=\"tensorboard\", push_to_hub=False,\n",
    "    alpha=0.5, temperature=4.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = DistillationTrainer(\n",
    "    student_model,\n",
    "    training_args,\n",
    "    teacher_model=teacher_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanjuncho/anaconda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 32470\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1778\n",
      "  Number of trainable parameters = 68117022\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1778' max='1778' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1778/1778 09:52, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.374700</td>\n",
       "      <td>1.375127</td>\n",
       "      <td>0.511526</td>\n",
       "      <td>0.511526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.626100</td>\n",
       "      <td>0.912751</td>\n",
       "      <td>0.637991</td>\n",
       "      <td>0.637991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.878040</td>\n",
       "      <td>0.649195</td>\n",
       "      <td>0.649195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.276200</td>\n",
       "      <td>0.832446</td>\n",
       "      <td>0.639665</td>\n",
       "      <td>0.639665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.199700</td>\n",
       "      <td>0.680994</td>\n",
       "      <td>0.657180</td>\n",
       "      <td>0.657180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.152400</td>\n",
       "      <td>0.561346</td>\n",
       "      <td>0.651771</td>\n",
       "      <td>0.651771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>0.457319</td>\n",
       "      <td>0.642756</td>\n",
       "      <td>0.642756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-254\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-254/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-254/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-254/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-254/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-1524] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-508\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-508/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-508/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-508/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-508/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-1778] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-762\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-762/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-762/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-762/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-762/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-254] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-1016\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-1016/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-1016/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-1016/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-1016/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-508] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-1270\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-1270/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-1270/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-1270/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-1270/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-762] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-1524\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-1524/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-1524/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-1524/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-1524/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-1016] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-1778\n",
      "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-1778/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-1778/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-1778/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-1778/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-1524] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distilroberta-base-sst2-distilled/checkpoint-1270 (score: 0.6571796522858983).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1778, training_loss=0.4506485824241681, metrics={'train_runtime': 592.6514, 'train_samples_per_second': 383.514, 'train_steps_per_second': 3.0, 'total_flos': 7530887358489600.0, 'train_loss': 0.4506485824241681, 'epoch': 7.0})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/61 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4472186863422394,\n",
       " 'eval_accuracy': 0.6571796522858983,\n",
       " 'eval_f1': 0.6571796522858983,\n",
       " 'eval_runtime': 10.1921,\n",
       " 'eval_samples_per_second': 761.864,\n",
       " 'eval_steps_per_second': 5.985,\n",
       " 'epoch': 7.0}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further deacreaing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_teacher = student_model\n",
    "#new_teacher = teacher_model.from_pretrained('./klue-roberta-base-re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/hanjuncho/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/hanjuncho/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    student,\n",
    "    num_labels=num_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config = student_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config.__dict__['num_hidden_layers'] = 3\n",
    "new_config.__dict__['num_labels'] = num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32004, 768)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model = AutoModelForSequenceClassification.from_config(new_config)\n",
    "student_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_weights = []\n",
    "for i, p in enumerate(student_model.parameters()):\n",
    "    student_weights.append(p)\n",
    "# initialized one layer out of two\n",
    "teacher_weights = []\n",
    "for i, p in enumerate(new_teacher.parameters()):\n",
    "    teacher_weights.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0037, -0.0081, -0.0167,  ...,  0.0285,  0.0038,  0.0092],\n",
       "        [ 0.0271, -0.0415, -0.0143,  ..., -0.0109,  0.0231, -0.0177],\n",
       "        [-0.0189, -0.0133,  0.0235,  ...,  0.0072,  0.0147, -0.0036],\n",
       "        ...,\n",
       "        [-0.0117,  0.0232, -0.0339,  ..., -0.0257,  0.0165, -0.0316],\n",
       "        [-0.0233,  0.0075,  0.0217,  ...,  0.0090,  0.0020,  0.0254],\n",
       "        [-0.0194,  0.0162, -0.0178,  ..., -0.0044,  0.0181, -0.0265]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First and last layers\n",
    "student_weights[0].data.copy_(teacher_weights[0].data)\n",
    "student_weights[1].data.copy_(teacher_weights[1].data)\n",
    "student_weights[2].data.copy_(teacher_weights[2].data)\n",
    "student_weights[-1].data.copy_(teacher_weights[-1].data)\n",
    "student_weights[-2].data.copy_(teacher_weights[-2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 3\n",
    "for i in range(6):\n",
    "    if i % 2 == 1:\n",
    "        std_idx = i // 2\n",
    "        for j in range(16):\n",
    "            student_weights[base+std_idx*16+j].data.copy_(teacher_weights[base+i*16+j].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self._move_model_to_device(self.teacher,self.model.device)\n",
    "        self.teacher.eval()\n",
    "        self.step = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        self.step += 1\n",
    "        # compute student output\n",
    "        outputs_student = model(**inputs)\n",
    "        student_loss=outputs_student.loss\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher(**inputs)\n",
    "\n",
    "        # assert size\n",
    "        assert outputs_student.logits.size() == outputs_teacher.logits.size()\n",
    "\n",
    "        # compute distillation loss and soften probabilities\n",
    "        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "#         cos_loss_function = nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
    "        \n",
    "        loss_logits = (loss_function(\n",
    "            F.log_softmax(outputs_student.logits / self.args.temperature, dim=-1),\n",
    "            F.softmax(outputs_teacher.logits / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n",
    "        # return weighted student loss\n",
    "        loss = 1.*(1778-self.step)/1778 * student_loss + (1. - 1.*(1778-self.step)/1778) * loss_logits\n",
    "        return (loss, outputs_student) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "### to continue learning\n",
    "\n",
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=\"distilroberta-base-sst2-distilled2\",\n",
    "    num_train_epochs=7, per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128, fp16=True, \n",
    "    learning_rate=6e-5, seed=33, \n",
    "    logging_dir=f\"distilroberta-base-sst2-distilled2/logs\",\n",
    "    logging_strategy=\"epoch\", evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", save_total_limit=2, \n",
    "    load_best_model_at_end=True, metric_for_best_model=\"eval_f1\", \n",
    "    report_to=\"tensorboard\", push_to_hub=False,\n",
    "    alpha=0.5, temperature=4.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = DistillationTrainer(\n",
    "    student_model,\n",
    "    training_args,\n",
    "    teacher_model=new_teacher, # changed for comparison\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanjuncho/anaconda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 32470\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1778\n",
      "  Number of trainable parameters = 46853406\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1778' max='1778' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1778/1778 05:20, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.260600</td>\n",
       "      <td>1.467542</td>\n",
       "      <td>0.518995</td>\n",
       "      <td>0.518995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.593200</td>\n",
       "      <td>1.267018</td>\n",
       "      <td>0.549646</td>\n",
       "      <td>0.549646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.381500</td>\n",
       "      <td>1.230793</td>\n",
       "      <td>0.560077</td>\n",
       "      <td>0.560077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>1.208514</td>\n",
       "      <td>0.523503</td>\n",
       "      <td>0.523503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.962329</td>\n",
       "      <td>0.561494</td>\n",
       "      <td>0.561494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>0.823961</td>\n",
       "      <td>0.537798</td>\n",
       "      <td>0.537798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.617718</td>\n",
       "      <td>0.552737</td>\n",
       "      <td>0.552737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled2/checkpoint-254\n",
      "Configuration saved in distilroberta-base-sst2-distilled2/checkpoint-254/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled2/checkpoint-254/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled2/checkpoint-254/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled2/checkpoint-254/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled2/checkpoint-762] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled2/checkpoint-508\n",
      "Configuration saved in distilroberta-base-sst2-distilled2/checkpoint-508/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled2/checkpoint-508/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled2/checkpoint-508/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled2/checkpoint-508/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled2/checkpoint-1778] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled2/checkpoint-762\n",
      "Configuration saved in distilroberta-base-sst2-distilled2/checkpoint-762/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled2/checkpoint-762/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled2/checkpoint-762/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled2/checkpoint-762/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled2/checkpoint-254] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled2/checkpoint-1016\n",
      "Configuration saved in distilroberta-base-sst2-distilled2/checkpoint-1016/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled2/checkpoint-1016/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled2/checkpoint-1016/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled2/checkpoint-1016/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled2/checkpoint-508] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled2/checkpoint-1270\n",
      "Configuration saved in distilroberta-base-sst2-distilled2/checkpoint-1270/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled2/checkpoint-1270/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled2/checkpoint-1270/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled2/checkpoint-1270/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled2/checkpoint-762] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled2/checkpoint-1524\n",
      "Configuration saved in distilroberta-base-sst2-distilled2/checkpoint-1524/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled2/checkpoint-1524/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled2/checkpoint-1524/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled2/checkpoint-1524/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled2/checkpoint-1016] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled2/checkpoint-1778\n",
      "Configuration saved in distilroberta-base-sst2-distilled2/checkpoint-1778/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled2/checkpoint-1778/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled2/checkpoint-1778/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled2/checkpoint-1778/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled2/checkpoint-1524] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distilroberta-base-sst2-distilled2/checkpoint-1270 (score: 0.5614938828074694).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1778, training_loss=0.4271300158162755, metrics={'train_runtime': 320.8228, 'train_samples_per_second': 708.46, 'train_steps_per_second': 5.542, 'total_flos': 3819137766958080.0, 'train_loss': 0.4271300158162755, 'epoch': 7.0})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/61 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7206025719642639,\n",
       " 'eval_accuracy': 0.592530585962653,\n",
       " 'eval_f1': 0.592530585962653,\n",
       " 'eval_runtime': 8.9517,\n",
       " 'eval_samples_per_second': 867.435,\n",
       " 'eval_steps_per_second': 6.814,\n",
       " 'epoch': 7.0}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student_model.state_dict(), './models/distilkoroberta_first_7epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning on Downstream Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = deepcopy(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset klue/nli (download: 1.20 MiB, generated: 6.10 MiB, post-processed: Unknown size, total: 7.30 MiB) to /home/seungjoonpark/.cache/huggingface/datasets/klue/nli/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.26M/1.26M [00:00<00:00, 6.19MB/s]\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset klue downloaded and prepared to /home/seungjoonpark/.cache/huggingface/datasets/klue/nli/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 422.51it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"klue\", 'nli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.76kB [00:00, 2.14MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"glue\", \"qnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ÌûõÍ±∏ ÏßÑÏã¨ ÏµúÍ≥†Îã§ Í∑∏ Ïñ¥Îñ§ ÌûàÏñ¥Î°úÎ≥¥Îã§ Î©ãÏßÄÎã§\n",
      "Sentence 2: ÌûõÍ±∏ ÏßÑÏã¨ ÏµúÍ≥†Î°ú Î©ãÏßÄÎã§.\n"
     ]
    }
   ],
   "source": [
    "sentence1_key, sentence2_key = (\"premise\", \"hypothesis\")\n",
    "print(f\"Sentence 1: {datasets['train'][0][sentence1_key]}\")\n",
    "print(f\"Sentence 2: {datasets['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 24/25 [00:01<00:00, 21.61ba/s]\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00, 15.01ba/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[sentence1_key],\n",
    "        examples[sentence2_key],\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "encoded_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"activation\": \"relu\",\n",
       "  \"attention_dropout\": 0.4,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"transformers_version\": \"4.23.1\",\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['classifier.weight', 'classifier.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = 3\n",
    "my_config = DistilBertConfig(activation=\"relu\", attention_dropout=0.4, vocab_size=32000, n_layers=6, num_labels=num_labels)\n",
    "model = AutoModelForSequenceClassification.from_config(my_config)\n",
    "model_dict = model.state_dict()\n",
    "pretrained_dict = torch.load(\"/home/seungjoonpark/DistilKoBERT/models/distilkoroberta.pt\")\n",
    "del pretrained_dict[next(reversed(pretrained_dict))]\n",
    "del pretrained_dict[next(reversed(pretrained_dict))]\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "model_dict.update(pretrained_dict) \n",
    "model.load_state_dict(pretrained_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "metric_name = \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"test-nli\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_datasets[\"train\"],\n",
    "    eval_dataset=encoded_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: source, hypothesis, premise, guid. If source, hypothesis, premise, guid are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/seungjoonpark/miniconda3/envs/nlp_project/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 24998\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 490\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='490' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [490/490 04:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.097590</td>\n",
       "      <td>0.380333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.113917</td>\n",
       "      <td>0.390667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.146158</td>\n",
       "      <td>0.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.126838</td>\n",
       "      <td>0.397333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.137593</td>\n",
       "      <td>0.393000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: source, hypothesis, premise, guid. If source, hypothesis, premise, guid are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to test-nli/checkpoint-98\n",
      "Configuration saved in test-nli/checkpoint-98/config.json\n",
      "Model weights saved in test-nli/checkpoint-98/pytorch_model.bin\n",
      "tokenizer config file saved in test-nli/checkpoint-98/tokenizer_config.json\n",
      "Special tokens file saved in test-nli/checkpoint-98/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: source, hypothesis, premise, guid. If source, hypothesis, premise, guid are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to test-nli/checkpoint-196\n",
      "Configuration saved in test-nli/checkpoint-196/config.json\n",
      "Model weights saved in test-nli/checkpoint-196/pytorch_model.bin\n",
      "tokenizer config file saved in test-nli/checkpoint-196/tokenizer_config.json\n",
      "Special tokens file saved in test-nli/checkpoint-196/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: source, hypothesis, premise, guid. If source, hypothesis, premise, guid are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to test-nli/checkpoint-294\n",
      "Configuration saved in test-nli/checkpoint-294/config.json\n",
      "Model weights saved in test-nli/checkpoint-294/pytorch_model.bin\n",
      "tokenizer config file saved in test-nli/checkpoint-294/tokenizer_config.json\n",
      "Special tokens file saved in test-nli/checkpoint-294/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: source, hypothesis, premise, guid. If source, hypothesis, premise, guid are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to test-nli/checkpoint-392\n",
      "Configuration saved in test-nli/checkpoint-392/config.json\n",
      "Model weights saved in test-nli/checkpoint-392/pytorch_model.bin\n",
      "tokenizer config file saved in test-nli/checkpoint-392/tokenizer_config.json\n",
      "Special tokens file saved in test-nli/checkpoint-392/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: source, hypothesis, premise, guid. If source, hypothesis, premise, guid are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to test-nli/checkpoint-490\n",
      "Configuration saved in test-nli/checkpoint-490/config.json\n",
      "Model weights saved in test-nli/checkpoint-490/pytorch_model.bin\n",
      "tokenizer config file saved in test-nli/checkpoint-490/tokenizer_config.json\n",
      "Special tokens file saved in test-nli/checkpoint-490/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from test-nli/checkpoint-392 (score: 0.3973333333333333).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=490, training_loss=0.9852519132653061, metrics={'train_runtime': 288.5789, 'train_samples_per_second': 433.122, 'train_steps_per_second': 1.698, 'total_flos': 2619227819706804.0, 'train_loss': 0.9852519132653061, 'epoch': 5.0})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: source, hypothesis, premise, guid. If source, hypothesis, premise, guid are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1056615114212036,\n",
       " 'eval_accuracy': 0.37133333333333335,\n",
       " 'eval_runtime': 2.453,\n",
       " 'eval_samples_per_second': 1222.99,\n",
       " 'eval_steps_per_second': 4.892,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erF--_J-iQYa"
   },
   "source": [
    "## Installing Optuna for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFqHvgR5iWZP"
   },
   "source": [
    "## Defining the Hyperparamater Space to be optimized over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "pwerkX6TL-Z3"
   },
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\n",
    "      \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 10),\n",
    "      \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3 ,log=True),\n",
    "      \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
    "      \"temperature\": trial.suggest_int(\"temperature\", 2, 30),\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a936pRODi43P"
   },
   "source": [
    "## Running the Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JBi0KiDuMBXP",
    "outputId": "febe0f20-83c0-45da-9420-03101e1a8c58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=6` with an incompatible id to label map: {'0': 'ITÍ≥ºÌïô', '1': 'Í≤ΩÏ†ú', '2': 'ÏÇ¨Ìöå', '3': 'ÏÉùÌôúÎ¨∏Ìôî', '4': 'ÏÑ∏Í≥Ñ', '5': 'Ïä§Ìè¨Ï∏†', '6': 'Ï†ïÏπò'}. The number of labels wil be overwritten to 7.\n",
      "Using cuda_amp half precision backend\n",
      "\u001b[32m[I 2022-10-27 23:39:03,618]\u001b[0m A new study created in memory with name: no-name-72755f35-ffe2-455f-abeb-7ef48083cfc8\u001b[0m\n",
      "Trial: {'num_train_epochs': 4, 'learning_rate': 0.0003356216196363318, 'alpha': 0.0038843531441111745, 'temperature': 28}\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/seungjoonpark/miniconda3/envs/nlp_project/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 45678\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1428\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1428' max='1428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1428/1428 03:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.007726</td>\n",
       "      <td>0.148018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.007775</td>\n",
       "      <td>0.148018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>0.148018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.007662</td>\n",
       "      <td>0.148018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-357\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-357/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-357/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-357/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-357/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-714\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-714/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-714/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-714/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-714/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-1071\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1071/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1071/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1071/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1071/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-0/checkpoint-714] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-1428\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1428/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1428/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1428/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1428/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-0/checkpoint-1071] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distilroberta-base-sst2-distilled/run-0/checkpoint-357 (score: 0.14801800812561766).\n",
      "\u001b[32m[I 2022-10-27 23:42:15,108]\u001b[0m Trial 0 finished with value: 0.14801800812561766 and parameters: {'num_train_epochs': 4, 'learning_rate': 0.0003356216196363318, 'alpha': 0.0038843531441111745, 'temperature': 28}. Best is trial 0 with value: 0.14801800812561766.\u001b[0m\n",
      "Trial: {'num_train_epochs': 10, 'learning_rate': 4.354784416636035e-05, 'alpha': 0.22918617625637505, 'temperature': 9}\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/seungjoonpark/miniconda3/envs/nlp_project/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 45678\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3570' max='3570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3570/3570 08:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.387900</td>\n",
       "      <td>0.372254</td>\n",
       "      <td>0.706929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.342700</td>\n",
       "      <td>0.365087</td>\n",
       "      <td>0.717580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.354984</td>\n",
       "      <td>0.779620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>0.358589</td>\n",
       "      <td>0.764577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.323200</td>\n",
       "      <td>0.364620</td>\n",
       "      <td>0.752718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.320600</td>\n",
       "      <td>0.358994</td>\n",
       "      <td>0.775338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.318300</td>\n",
       "      <td>0.357679</td>\n",
       "      <td>0.784122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.316800</td>\n",
       "      <td>0.363774</td>\n",
       "      <td>0.765565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.315700</td>\n",
       "      <td>0.362860</td>\n",
       "      <td>0.767541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.314700</td>\n",
       "      <td>0.361569</td>\n",
       "      <td>0.772812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-357\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-357/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-357/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-357/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-357/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-714\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-714/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-714/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-714/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-714/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-1071\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1071/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1071/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1071/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1071/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-1/checkpoint-357] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-1428\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1428/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1428/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1428/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1428/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-1/checkpoint-714] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-1785\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1785/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1785/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1785/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1785/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-1/checkpoint-1428] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-2142\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2142/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2142/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2142/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2142/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-1/checkpoint-1785] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-2499\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2499/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2499/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2499/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2499/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-1/checkpoint-1071] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-2856\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2856/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2856/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2856/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2856/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-1/checkpoint-2142] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-3213\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-3213/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-3213/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-3213/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-3213/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-1/checkpoint-2856] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-3570\n",
      "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-3570/config.json\n",
      "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-3570/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-3570/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-3570/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-1/checkpoint-3213] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distilroberta-base-sst2-distilled/run-1/checkpoint-2499 (score: 0.7841221038761392).\n",
      "\u001b[32m[I 2022-10-27 23:50:18,572]\u001b[0m Trial 1 finished with value: 0.7728121225430987 and parameters: {'num_train_epochs': 10, 'learning_rate': 4.354784416636035e-05, 'alpha': 0.22918617625637505, 'temperature': 9}. Best is trial 1 with value: 0.7728121225430987.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestRun(run_id='1', objective=0.7728121225430987, hyperparameters={'num_train_epochs': 10, 'learning_rate': 4.354784416636035e-05, 'alpha': 0.22918617625637505, 'temperature': 9})\n"
     ]
    }
   ],
   "source": [
    "my_config = DistilBertConfig(activation=\"relu\", attention_dropout=0.4, vocab_size=32000, n_layers=6, num_labels=6,\n",
    "                            label2id=label2id, id2label=id2label)\n",
    "\n",
    "\n",
    "def student_init():\n",
    "    return AutoModelForSequenceClassification.from_config(\n",
    "    my_config)\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "    model_init=student_init,\n",
    "    args=training_args,\n",
    "    teacher_model=teacher_model,\n",
    "    train_dataset=sst2_enc[\"train\"],\n",
    "    eval_dataset=sst2_enc[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    n_trials=2,\n",
    "    direction=\"maximize\",\n",
    "    hp_space=hp_space\n",
    ")\n",
    "\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8X7E7FpjFg-"
   },
   "source": [
    "## Updating the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "UqW7rvoNMil5"
   },
   "outputs": [],
   "source": [
    "# overwriting the previous hyperparameters\n",
    "for k,v in best_run.hyperparameters.items():\n",
    "    setattr(training_args, k, v)\n",
    "\n",
    "# new repository\n",
    "best_model_ckpt = \"distilroberta-best\"\n",
    "training_args.output_dir = best_model_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNhUWuyOjTYu"
   },
   "source": [
    "## Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v1VJCtxtMjnJ",
    "outputId": "0f2c24eb-6388-4595-a4fd-cd43c7f2bc2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/seungjoonpark/miniconda3/envs/nlp_project/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 45678\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3570' max='3570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3570/3570 08:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.330300</td>\n",
       "      <td>0.359125</td>\n",
       "      <td>0.763918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.362589</td>\n",
       "      <td>0.753596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.361970</td>\n",
       "      <td>0.765345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.366088</td>\n",
       "      <td>0.753926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>0.361465</td>\n",
       "      <td>0.772263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.316400</td>\n",
       "      <td>0.365798</td>\n",
       "      <td>0.762271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.367955</td>\n",
       "      <td>0.753267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.363063</td>\n",
       "      <td>0.773032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.313200</td>\n",
       "      <td>0.364926</td>\n",
       "      <td>0.767761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.364964</td>\n",
       "      <td>0.767212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-best/checkpoint-357\n",
      "Configuration saved in distilroberta-best/checkpoint-357/config.json\n",
      "Model weights saved in distilroberta-best/checkpoint-357/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-best/checkpoint-357/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-best/checkpoint-357/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-best/checkpoint-714\n",
      "Configuration saved in distilroberta-best/checkpoint-714/config.json\n",
      "Model weights saved in distilroberta-best/checkpoint-714/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-best/checkpoint-714/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-best/checkpoint-714/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-best/checkpoint-1071\n",
      "Configuration saved in distilroberta-best/checkpoint-1071/config.json\n",
      "Model weights saved in distilroberta-best/checkpoint-1071/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-best/checkpoint-1071/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-best/checkpoint-1071/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-best/checkpoint-357] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-best/checkpoint-1428\n",
      "Configuration saved in distilroberta-best/checkpoint-1428/config.json\n",
      "Model weights saved in distilroberta-best/checkpoint-1428/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-best/checkpoint-1428/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-best/checkpoint-1428/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-best/checkpoint-714] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-best/checkpoint-1785\n",
      "Configuration saved in distilroberta-best/checkpoint-1785/config.json\n",
      "Model weights saved in distilroberta-best/checkpoint-1785/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-best/checkpoint-1785/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-best/checkpoint-1785/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-best/checkpoint-1071] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-best/checkpoint-2142\n",
      "Configuration saved in distilroberta-best/checkpoint-2142/config.json\n",
      "Model weights saved in distilroberta-best/checkpoint-2142/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-best/checkpoint-2142/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-best/checkpoint-2142/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-best/checkpoint-1428] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-best/checkpoint-2499\n",
      "Configuration saved in distilroberta-best/checkpoint-2499/config.json\n",
      "Model weights saved in distilroberta-best/checkpoint-2499/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-best/checkpoint-2499/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-best/checkpoint-2499/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-best/checkpoint-2142] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-best/checkpoint-2856\n",
      "Configuration saved in distilroberta-best/checkpoint-2856/config.json\n",
      "Model weights saved in distilroberta-best/checkpoint-2856/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-best/checkpoint-2856/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-best/checkpoint-2856/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-best/checkpoint-1785] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-best/checkpoint-3213\n",
      "Configuration saved in distilroberta-best/checkpoint-3213/config.json\n",
      "Model weights saved in distilroberta-best/checkpoint-3213/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-best/checkpoint-3213/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-best/checkpoint-3213/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-best/checkpoint-2499] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: title, date, token_type_ids, guid, url. If title, date, token_type_ids, guid, url are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9107\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to distilroberta-best/checkpoint-3570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in distilroberta-best/checkpoint-3570/config.json\n",
      "Model weights saved in distilroberta-best/checkpoint-3570/pytorch_model.bin\n",
      "tokenizer config file saved in distilroberta-best/checkpoint-3570/tokenizer_config.json\n",
      "Special tokens file saved in distilroberta-best/checkpoint-3570/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-best/checkpoint-3213] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distilroberta-best/checkpoint-2856 (score: 0.7730317338311189).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3570, training_loss=0.3187510471717984, metrics={'train_runtime': 482.5227, 'train_samples_per_second': 946.65, 'train_steps_per_second': 7.399, 'total_flos': 2774037043248840.0, 'train_loss': 0.3187510471717984, 'epoch': 10.0})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New Trainer with the updated parameters\n",
    "optimal_trainer = DistillationTrainer(\n",
    "    student_model,\n",
    "    training_args,\n",
    "    teacher_model=teacher_model,\n",
    "    train_dataset=sst2_enc[\"train\"],\n",
    "    eval_dataset=sst2_enc[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "optimal_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMDSPGmMhUYmks1I9B5d2fo",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09db8fe0b55443fdab2ac54e84bb7ff5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_861ab3f25cb4479f9ace3978dc315ecf",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_52eabebd19834610890b10347acaf05a",
      "value": 3
     }
    },
    "0a8902cb735d4f658fd387204149ace6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "0faf64a0c2af4340ac3b0eed24732d5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0614b4ade314c5c8073193bb1d3408f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_38b21b8459234a42b8fb69752644d48a",
      "value": "100%"
     }
    },
    "135dbaaa41a946ca899dc8164bca3e44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ad3705618644447bfcfdb9b65271883",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b71e32972c104b53a669ec7591e557ff",
      "value": "100%"
     }
    },
    "155517c19bc24d599b068af7e28ad8cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e5da5a29aaa4700bcdbdfc768b33aaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3838ac5a07244d7da893c8ba71b21fd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "38b21b8459234a42b8fb69752644d48a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b32dbed334941b881ae0e563d7c40ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a8902cb735d4f658fd387204149ace6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e25584843ddd4a4484ec94e7e6c0f6a0",
      "value": 0
     }
    },
    "3ddd98a8524042adba2bbaab1dbf60af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5293a6fc609849eda02622c6fa5b2167": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "52eabebd19834610890b10347acaf05a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5ad3705618644447bfcfdb9b65271883": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ffb8a1db11a43cb8602ec7af8c55f77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6abc6629a8b0412c9b09ad02c3376127": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_155517c19bc24d599b068af7e28ad8cf",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3838ac5a07244d7da893c8ba71b21fd9",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "7874b0b368c24ef7b15fac3a4ed83582": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9403580e4524754980170c5f1130bd9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_1e5da5a29aaa4700bcdbdfc768b33aaf",
      "value": " 1/1 [00:00&lt;00:00,  8.90ba/s]"
     }
    },
    "861ab3f25cb4479f9ace3978dc315ecf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8722d84268664df5a8012a83afd575f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a1e9c4517dc463ba78b91b16431008a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c2682025ce14a88b39dbfc826e33954": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e2728489569432394255a3bb3a80247": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_af6bdd67d22a42f0be80845511720ae0",
       "IPY_MODEL_3b32dbed334941b881ae0e563d7c40ce",
       "IPY_MODEL_6abc6629a8b0412c9b09ad02c3376127"
      ],
      "layout": "IPY_MODEL_8722d84268664df5a8012a83afd575f3"
     }
    },
    "a255def76d9b4c9db5d9cc7385fc9e6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a864568c3b89405cb3ca8e36180f9e61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af6bdd67d22a42f0be80845511720ae0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ddd98a8524042adba2bbaab1dbf60af",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8a1e9c4517dc463ba78b91b16431008a",
      "value": ""
     }
    },
    "b0614b4ade314c5c8073193bb1d3408f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b591e297adff46fa8df014fe9192caa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e39bf65dc61a4836805a0a8bd23503a3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a864568c3b89405cb3ca8e36180f9e61",
      "value": " 3/3 [00:00&lt;00:00, 66.64it/s]"
     }
    },
    "b71e32972c104b53a669ec7591e557ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9403580e4524754980170c5f1130bd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cde2d6e48a7748ca840c745e52fa06c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ffb8a1db11a43cb8602ec7af8c55f77",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5293a6fc609849eda02622c6fa5b2167",
      "value": 1
     }
    },
    "e25584843ddd4a4484ec94e7e6c0f6a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e39bf65dc61a4836805a0a8bd23503a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9acdb1a1d2643819339b8f0cf3cecd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_135dbaaa41a946ca899dc8164bca3e44",
       "IPY_MODEL_09db8fe0b55443fdab2ac54e84bb7ff5",
       "IPY_MODEL_b591e297adff46fa8df014fe9192caa3"
      ],
      "layout": "IPY_MODEL_8c2682025ce14a88b39dbfc826e33954"
     }
    },
    "ea941579121247548fdf7774079f2f5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0faf64a0c2af4340ac3b0eed24732d5a",
       "IPY_MODEL_cde2d6e48a7748ca840c745e52fa06c2",
       "IPY_MODEL_7874b0b368c24ef7b15fac3a4ed83582"
      ],
      "layout": "IPY_MODEL_a255def76d9b4c9db5d9cc7385fc9e6d"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
